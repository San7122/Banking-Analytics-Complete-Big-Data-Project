{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banking Data Analytics - Spark Data Processing & ML\n",
    "## Part 1: EDA + Part 4: Machine Learning\n",
    "\n",
    "This notebook can be run directly in Google Colab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Install PySpark in Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PySpark\n",
    "!pip install pyspark -q\n",
    "print(\"PySpark installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Banking Analytics\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark Session created!\")\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Dataset\n",
    "Upload the `bank.csv` file when prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab - Upload file\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "print(\"File uploaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = spark.read.csv(\"bank.csv\", header=True, inferSchema=True)\n",
    "print(f\"Dataset loaded: {df.count()} rows, {len(df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1: SPARK DATA PROCESSING (EDA)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Data Loading and Basic Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first 10 rows\n",
    "print(\"=\" * 60)\n",
    "print(\"TASK 1: DATA LOADING AND BASIC INSPECTION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n--- First 10 Rows ---\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print schema\n",
    "print(\"\\n--- Schema ---\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"\\n--- Summary Statistics ---\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Data Filtering and Column Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TASK 2: DATA FILTERING AND COLUMN OPERATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Filter clients with balance > 1000\n",
    "print(\"\\n--- Clients with Balance > 1000 ---\")\n",
    "df_filtered = df.filter(col(\"balance\") > 1000)\n",
    "print(f\"Total clients with balance > 1000: {df_filtered.count()}\")\n",
    "df_filtered.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add quarter column\n",
    "print(\"\\n--- Adding Quarter Column ---\")\n",
    "df_with_quarter = df.withColumn(\n",
    "    \"quarter\",\n",
    "    when(col(\"month\").isin(['jan', 'feb', 'mar']), 1)\n",
    "    .when(col(\"month\").isin(['apr', 'may', 'jun']), 2)\n",
    "    .when(col(\"month\").isin(['jul', 'aug', 'sep']), 3)\n",
    "    .otherwise(4)\n",
    ")\n",
    "df_with_quarter.select(\"month\", \"quarter\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: GroupBy and Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TASK 3: GROUPBY AND AGGREGATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Average balance by job type\n",
    "print(\"\\n--- Average Balance and Age by Job Type ---\")\n",
    "job_stats = df.groupBy(\"job\").agg(\n",
    "    round(avg(\"balance\"), 2).alias(\"avg_balance\"),\n",
    "    round(avg(\"age\"), 2).alias(\"avg_age\")\n",
    ").orderBy(col(\"avg_balance\").desc())\n",
    "job_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscribed clients by marital status\n",
    "print(\"\\n--- Subscribed Clients by Marital Status ---\")\n",
    "marital_sub = df.filter(col(\"y\") == \"yes\").groupBy(\"marital\").count().orderBy(col(\"count\").desc())\n",
    "marital_sub.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: UDF for Age Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TASK 4: UDF TO CATEGORIZE AGE GROUPS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Define UDF\n",
    "def categorize_age(age):\n",
    "    if age < 30:\n",
    "        return \"<30\"\n",
    "    elif age <= 60:\n",
    "        return \"30-60\"\n",
    "    else:\n",
    "        return \">60\"\n",
    "\n",
    "age_group_udf = udf(categorize_age, StringType())\n",
    "\n",
    "# Apply UDF\n",
    "df_with_age = df.withColumn(\"age_group\", age_group_udf(col(\"age\")))\n",
    "print(\"\\n--- Age Group Distribution ---\")\n",
    "df_with_age.groupBy(\"age_group\").count().orderBy(\"age_group\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Advanced Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TASK 5: ADVANCED DATA TRANSFORMATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Subscription rate by education\n",
    "print(\"\\n--- Subscription Rate by Education ---\")\n",
    "edu_stats = df.groupBy(\"education\").agg(\n",
    "    count(\"*\").alias(\"total\"),\n",
    "    sum(when(col(\"y\") == \"yes\", 1).otherwise(0)).alias(\"subscribed\")\n",
    ")\n",
    "edu_stats = edu_stats.withColumn(\n",
    "    \"subscription_rate_pct\",\n",
    "    round((col(\"subscribed\") / col(\"total\")) * 100, 2)\n",
    ").orderBy(col(\"subscription_rate_pct\").desc())\n",
    "edu_stats.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 3 jobs with highest default rate\n",
    "print(\"\\n--- Top 3 Jobs with Highest Default Rate ---\")\n",
    "default_stats = df.groupBy(\"job\").agg(\n",
    "    count(\"*\").alias(\"total\"),\n",
    "    sum(when(col(\"default\") == \"yes\", 1).otherwise(0)).alias(\"defaulters\")\n",
    ")\n",
    "default_stats = default_stats.withColumn(\n",
    "    \"default_rate_pct\",\n",
    "    round((col(\"defaulters\") / col(\"total\")) * 100, 2)\n",
    ").orderBy(col(\"default_rate_pct\").desc())\n",
    "default_stats.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TASK 6: STRING MANIPULATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Concatenate job and marital\n",
    "print(\"\\n--- Concatenating Job and Marital ---\")\n",
    "df_concat = df.withColumn(\"job_marital\", concat(col(\"job\"), lit(\"_\"), col(\"marital\")))\n",
    "df_concat.select(\"job\", \"marital\", \"job_marital\").show(10)\n",
    "\n",
    "# Uppercase contact\n",
    "print(\"\\n--- Contact in Uppercase ---\")\n",
    "df.withColumn(\"contact_upper\", upper(col(\"contact\"))).select(\"contact\", \"contact_upper\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TASK 7: DATA VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "job_counts = df.groupBy(\"job\").count().orderBy(col(\"count\").desc()).toPandas()\n",
    "\n",
    "# Create bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(job_counts['job'], job_counts['count'], color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Job Type', fontsize=12)\n",
    "plt.ylabel('Number of Clients', fontsize=12)\n",
    "plt.title('Distribution of Clients by Job Type', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8: Complex Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TASK 8: COMPLEX QUERIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Month analysis\n",
    "print(\"\\n--- Monthly Contact Analysis ---\")\n",
    "monthly = df.groupBy(\"month\").agg(\n",
    "    count(\"*\").alias(\"total_contacts\"),\n",
    "    sum(when(col(\"y\") == \"yes\", 1).otherwise(0)).alias(\"subscribed\")\n",
    ")\n",
    "monthly = monthly.withColumn(\n",
    "    \"success_rate_pct\",\n",
    "    round((col(\"subscribed\") / col(\"total_contacts\")) * 100, 2)\n",
    ").orderBy(col(\"total_contacts\").desc())\n",
    "monthly.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average duration by subscription\n",
    "print(\"\\n--- Average Duration: Subscribed vs Not ---\")\n",
    "df.groupBy(\"y\").agg(round(avg(\"duration\"), 2).alias(\"avg_duration_sec\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TASK 9: CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "correlation = df.select(corr(\"age\", \"balance\")).collect()[0][0]\n",
    "print(f\"\\nCorrelation between Age and Balance: {correlation:.4f}\")\n",
    "print(\"Interpretation: Weak/no linear relationship\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 10: Default Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TASK 10: DEFAULT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "default_dist = df.groupBy(\"default\").count().toPandas()\n",
    "print(default_dist)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(default_dist['default'], default_dist['count'], color=['green', 'red'])\n",
    "plt.xlabel('Credit Default Status')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Credit Default Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 11: Contact Method Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TASK 11: CONTACT METHOD ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "contact_stats = df.groupBy(\"contact\").agg(\n",
    "    count(\"*\").alias(\"total\"),\n",
    "    sum(when(col(\"y\") == \"yes\", 1).otherwise(0)).alias(\"subscribed\")\n",
    ")\n",
    "contact_stats = contact_stats.withColumn(\n",
    "    \"success_rate_pct\",\n",
    "    round((col(\"subscribed\") / col(\"total\")) * 100, 2)\n",
    ").orderBy(col(\"success_rate_pct\").desc())\n",
    "contact_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 12: Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TASK 12: SPARK SQL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create temp view\n",
    "df_with_age.createOrReplaceTempView(\"bank_data\")\n",
    "\n",
    "# SQL Query 1\n",
    "print(\"\\n--- SQL: Average Balance by Age Group ---\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT age_group, \n",
    "           ROUND(AVG(balance), 2) as avg_balance,\n",
    "           COUNT(*) as count\n",
    "    FROM bank_data\n",
    "    GROUP BY age_group\n",
    "    ORDER BY age_group\n",
    "\"\"\").show()\n",
    "\n",
    "# SQL Query 2\n",
    "print(\"\\n--- SQL: Top 5 Job Types ---\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT job, COUNT(*) as count\n",
    "    FROM bank_data\n",
    "    GROUP BY job\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 4: SPARK ML - MACHINE LEARNING\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 & 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SPARK ML: DATA PREPROCESSING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n--- Missing Values Check ---\")\n",
    "for c in df.columns:\n",
    "    missing = df.filter(col(c).isNull()).count()\n",
    "    if missing > 0:\n",
    "        print(f\"{c}: {missing}\")\n",
    "print(\"No missing values found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers (IQR capping)\n",
    "print(\"\\n--- Handling Outliers ---\")\n",
    "numerical_cols = ['age', 'balance', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "def cap_outliers(df, column):\n",
    "    quantiles = df.approxQuantile(column, [0.25, 0.75], 0.05)\n",
    "    Q1, Q3 = quantiles[0], quantiles[1]\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    return df.withColumn(column,\n",
    "        when(col(column) < lower, lower)\n",
    "        .when(col(column) > upper, upper)\n",
    "        .otherwise(col(column)))\n",
    "\n",
    "for c in numerical_cols:\n",
    "    df = cap_outliers(df, c)\n",
    "print(f\"Outliers capped for: {numerical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Categorical columns\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', \n",
    "                    'loan', 'contact', 'month', 'poutcome']\n",
    "\n",
    "# Create indexers and encoders\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\") \n",
    "            for c in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_enc\") \n",
    "            for c in categorical_cols]\n",
    "\n",
    "# Label indexer\n",
    "label_indexer = StringIndexer(inputCol=\"y\", outputCol=\"label\")\n",
    "\n",
    "# Feature assembler\n",
    "feature_cols = numerical_cols + [c+\"_enc\" for c in categorical_cols]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\")\n",
    "\n",
    "# Scaler\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
    "\n",
    "print(f\"Feature columns: {len(feature_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"Training: {train_data.count()}, Test: {test_data.count()}\")\n",
    "\n",
    "# Define models\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=100)\n",
    "dt = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, seed=42)\n",
    "\n",
    "# Pipeline stages\n",
    "stages = indexers + encoders + [label_indexer, assembler, scaler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "models = {\"Logistic Regression\": lr, \"Decision Tree\": dt, \"Random Forest\": rf}\n",
    "results = {}\n",
    "\n",
    "for name, classifier in models.items():\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    pipeline = Pipeline(stages=stages + [classifier])\n",
    "    model = pipeline.fit(train_data)\n",
    "    predictions = model.transform(test_data)\n",
    "    \n",
    "    # Evaluate\n",
    "    auc = BinaryClassificationEvaluator(labelCol=\"label\").evaluate(predictions)\n",
    "    acc = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\").evaluate(predictions)\n",
    "    f1 = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\").evaluate(predictions)\n",
    "    \n",
    "    results[name] = {\"AUC\": auc, \"Accuracy\": acc, \"F1\": f1, \"model\": model}\n",
    "    print(f\"  AUC: {auc:.4f}, Accuracy: {acc:.4f}, F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Model Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"{'Model':<25} {'AUC':<10} {'Accuracy':<10} {'F1':<10}\")\n",
    "print(\"-\" * 55)\n",
    "for name, metrics in results.items():\n",
    "    print(f\"{name:<25} {metrics['AUC']:.4f}     {metrics['Accuracy']:.4f}     {metrics['F1']:.4f}\")\n",
    "\n",
    "best = max(results, key=lambda x: results[x]['AUC'])\n",
    "print(f\"\\n*** Best Model: {best} ***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf_tune = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
    "pipeline_rf = Pipeline(stages=stages + [rf_tune])\n",
    "\n",
    "# Parameter grid\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf_tune.numTrees, [50, 100]) \\\n",
    "    .addGrid(rf_tune.maxDepth, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "# Cross-validator\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline_rf,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=BinaryClassificationEvaluator(labelCol=\"label\"),\n",
    "    numFolds=3,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Running cross-validation...\")\n",
    "cv_model = cv.fit(train_data)\n",
    "\n",
    "# Evaluate\n",
    "cv_predictions = cv_model.transform(test_data)\n",
    "tuned_auc = BinaryClassificationEvaluator(labelCol=\"label\").evaluate(cv_predictions)\n",
    "print(f\"\\nTuned Model AUC: {tuned_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf_model = results[\"Random Forest\"][\"model\"].stages[-1]\n",
    "importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "# Create DataFrame\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols[:len(importances)],\n",
    "    'importance': importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "top10 = importance_df.head(10)\n",
    "plt.barh(top10['feature'], top10['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PROJECT COMPLETED!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\"\"\n",
    "Key Findings:\n",
    "1. Best Model: {best} with AUC {results[best]['AUC']:.4f}\n",
    "2. After tuning: AUC {tuned_auc:.4f}\n",
    "3. Most important features: duration, poutcome, contact\n",
    "\n",
    "Business Insights:\n",
    "- Longer calls increase subscription probability\n",
    "- Previous successful campaigns matter\n",
    "- Cellular contact is most effective\n",
    "\"\"\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
